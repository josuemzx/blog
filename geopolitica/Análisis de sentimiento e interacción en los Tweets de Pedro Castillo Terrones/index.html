<!DOCTYPE html>
<!--

    888                                      888   |
    888  e88~-_   d88~\ 888  888  e88~~8e    888___| 888  888   /~~~8e  888-~88e-~88e   /~~~8e  888-~88e
    888 d888   i C888   888  888 d888  88b   888   | 888  888       88b 888  888  888       88b 888  888
    888 8888   |  Y88b  888  888 8888__888   888   | 888  888  e88~-888 888  888  888  e88~-888 888  888
|   88P Y888   '   888D 888  888 Y888    ,   888   | 888  888 C888  888 888  888  888 C888  888 888  888
 \__8"   "88_-~  \_88P  "88_-888  "88___/    888   | "88_-888  "88_-888 888  888  888  "88_-888 888  888

-->
<html lang="es">

<meta http-equiv="content-type" content="text/html;charset=utf-8" />
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Análisis de sentimiento e interacción en los Tweets de Pedro Castillo Terrones</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <link rel="stylesheet" type="text/css" href="../../assets/styles/allc205.css?v=faca4a392d" />
  
  <script type="text/javascript">document.documentElement.className = 'js';</script>
  <link rel="icon" type="image/svg+xml" href="../../cdn/media/favicon.png">
  <link rel="mask-icon" href="../../assets/images/mask-iconc205.svg?v=faca4a392d" color="#000000">
  <link rel="icon" href="../../cdn/media/favicon.png" type="image/png" />
    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Josue Huaman" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Análisis de sentimiento e interacción en los Tweets de Pedro Castillo Terrones" />
    <meta property="og:description" content="Este estudio pretende descubrir la carga sentimental que poseen los tweets de Pedro Castillo y cuáles logran una mayor interacción con la población, para ello utilizaremos el entorno y lenguaje de programación R." />
   
   
    <meta property="og:url" content="https://openai.com/blog/triton/" />
    <meta property="og:image" content="https://openai.com/content/images/2021/07/triton-og-image.png" />
    <meta property="article:published_time" content="2021-07-28T16:00:00.000Z" />
    <meta property="article:modified_time" content="2021-08-05T19:05:41.000Z" />
    <meta property="article:tag" content="Research" />




    
    <meta property="article:publisher" content="https://www.facebook.com/josuehuaman" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Análisis de sentimiento e interacción en los Tweets de Pedro Castillo Terrones" />
    <meta name="twitter:description" content="Este estudio pretende descubrir la carga sentimental que poseen los tweets de Pedro Castillo y cuáles logran una mayor interacción con la población, para ello utilizaremos el entorno y lenguaje de programación R." />

    
    <meta name="generator" content="Ghost 5.22" />
    <link rel="alternate" type="application/rss+xml" title="OpenAI" href="../rss/index.html" />
    <script defer src="../../../cdn.jsdelivr.net/ghost/portal%40_2.19/umd/portal.min.js" data-ghost="https://openai.com/" data-key="23f7e2ddba37b787ce1ea90e77" data-api="https://openaidev.ghost.io/ghost/api/content/" crossorigin="anonymous"></script><style id="gh-members-styles">.gh-post-upgrade-cta-content,
.gh-post-upgrade-cta {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    text-align: center;
    width: 100%;
    color: #ffffff;
    font-size: 16px;
}

.gh-post-upgrade-cta-content {
    border-radius: 8px;
    padding: 40px 4vw;
}

.gh-post-upgrade-cta h2 {
    color: #ffffff;
    font-size: 28px;
    letter-spacing: -0.2px;
    margin: 0;
    padding: 0;
}

.gh-post-upgrade-cta p {
    margin: 20px 0 0;
    padding: 0;
}

.gh-post-upgrade-cta small {
    font-size: 16px;
    letter-spacing: -0.2px;
}

.gh-post-upgrade-cta a {
    color: #ffffff;
    cursor: pointer;
    font-weight: 500;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a:hover {
    color: #ffffff;
    opacity: 0.8;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a.gh-btn {
    display: block;
    background: #ffffff;
    text-decoration: none;
    margin: 28px 0 0;
    padding: 8px 18px;
    border-radius: 4px;
    font-size: 16px;
    font-weight: 600;
}

.gh-post-upgrade-cta a.gh-btn:hover {
    opacity: 0.92;
}</style>
    <script defer src="../../../cdn.jsdelivr.net/ghost/sodo-search%40_1.1/umd/sodo-search.min.js" data-key="23f7e2ddba37b787ce1ea90e77" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://openaidev.ghost.io/" crossorigin="anonymous"></script>
    <script defer src="../../public/cards.minc205.js?v=faca4a392d"></script>
    <link rel="stylesheet" type="text/css" href="../../public/cards.minc205.css?v=faca4a392d">
    <script defer src="../../public/member-attribution.minc205.js?v=faca4a392d"></script><style>:root {--ghost-accent-color: #15171A;}</style>
</head>
<body>
  
<article class="post" id="post-triton">

  
  
  
  
  <header>
    <nav class="nav container" data-url="/tags/geopolítica/">
    <div class="nav-row row align-items-center">
      <div class="d-none d-sm-block col-sm nav-symbol-wrap">
        <a href="../../index.html" class="nav-symbol"><IMG SRC="../../cdn/Logo-josue-b.png"></a>
      </div>
      <div class="col col-sm-auto">
        <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
          <div class="d-sm-none nav-symbol-wrap">
            <a href="../../index.html" class="nav-symbol"><IMG SRC="../../cdn/Logo-josue-bM.png"></a>
          </div>               
  
          <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="https://josuem.substack.com/p/recursos-abiertos-para-estudiar-data" data-slug="about">Cursos</a></li>
  
           <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="https://josuem.substack.com/p/libros" data-slug="about">Libros</a></li>
  
           <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="../cdn/media/RESUME.pdf" data-slug="about">CV</a></li>
  
           <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="../about/index.html" data-slug="about">About</a></li>
         </ul>
    </div>
  </div>
</nav>


  
          <div class="container mt-5">
    <div class="row">
      
        <div class="col-12 col-md-9 col-lg-8 col-xl-12 text-xl-center">
          <div class="max-width-xxwide mx-xl-auto">
            <h1 class="
   balance-text
  
  
  
  
  
  
  
   mb-0.75 mb-xl-2.25
  ">Análisis de sentimiento e interacción en los Tweets de Pedro Castillo Terrones</h1>
          </div>
        </div>
    </div>
      <div class="post-header-no-excerpt-spacer"></div>
    
  </div>

  
</header>

  <section class="container">
  <div class="row">
    <section class="content">
      
            <aside class="aside">  <div class="post-header-date small-copy color-fg-50 post-header-date--content mb-1.5">
    <time datetime="2021-07-28">Octubre 18, 2022</time>
    <div class="reading-time">14 minute read</div>
  </div>
</aside>
      
      <div class="js-post-content">
        <!--kg-card-begin: markdown--><p>Este estudio pretende descubrir la carga sentimental que poseen los tweets de Pedro Castillo y cuáles logran una mayor interacción con la población, para ello utilizaremos el entorno y lenguaje de programación R.</p>
<div class="btns">
<p><a href="https://github.com/openai/triton" class="btn btn-padded btn-dark btn-circle icon-code">View Code</a></p>
</div>
<hr>
<h2 id="Herramientas">Herramientas</h2>
<ul>
<li>R</li>
<li>rtweet</li>
<li>ggplot2</li>
</ul>
<h2 id="Recolección-de-datos">Recolección de datos</h2>
<p>Los datos serán obtenidos mediante la API de Twitter con la librería rtweet, para ello es necesario registrarse en la aplicación y obtener las credenciales en la sección desarrolladores, de forma predeterminada solo se puede acceder a información pública de los tweets y usuarios.</p>

<pre><code class="language-r">twitter_tokens 
  <- create_token(app = "", consumer_key = "", consumer_secret = "")
 </code></pre>
<div class="caption">Acceso a la API.</div>


<p>Una vez autenticados pasamos a recolectar los tweets.</p>
<pre><code class="language-r">castillovar <- get_timeline("PedroCastilloTe", n = 4000)</code></pre>

<p>Seleccionaremos la variable de interés y guardamos los datos en un archivo .csv:</p>
<pre><code class="language-r">castillovar <- castillovar[c("created_at", "text", "favorite_count", "retweet_count")]</code></pre>

<pre><code class="language-r">
  num_rows = nrow(castillovar)
  
  #Crear vector de columna ID
  X <- c(1:num_rows)
   
  #Enlazar la columna ID al marco de datos
  castillovar <- cbind(X , castillovar)</code></pre>

<pre><code class="language-r">write_as_csv(castillovar, "castillovar.csv")</code></pre>

<p>Con 'readr' cargamos el archivo .csv</p>
<pre><code class="language-r">library(readr)

castillovar <- read_csv("castillovar.csv")
castillovar</code></pre>

<h2 id="Preparacion-de-los-datos">Preparación de los datos</h2>

<p>Una pieza central para un análisis de texto (text mining) es el tokenizado. La tokenización es una forma de separar un fragmento de texto en unidades más pequeñas llamadas tokens. Aquí, los tokens pueden ser palabras, caracteres o subpalabras. Por lo tanto, la tokenización se puede clasificar ampliamente en 3 tipos: tokenización de palabra, carácter y subpalabra (caracteres n-grama).</p>
<p>Utilizaremos 'string' y 'stringr' para la manipulación de caracteres, 'dplyr' para la manipulación del data frame, magrittr para el uso de pipes y tidytext para el tokenizado y el análisis de texto.</p>

<pre><code class="language-r">library(stringi)
library(stringr)
library(dplyr)
library(magrittr)
library(tidytext)

# Quitamos los acentos a los tweets.
castillovar$text <- stri_trans_general(castillovar$text, "Latin-ASCII")

# Limpiamos la variable text de algunos elementos innecesarios y  tokenizamos.
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
castillovar_t <- castillovar %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = unnest_reg)  # Tokenizado
rm(replace_reg, unnest_reg) 

</code></pre>

<p>El resultado es DataFrame más grande, de 42751 observaciones, Donde se reemplaza la columna (variable) <i>text</i> por <i>word</i>, manteniendo la variable X como ID:</p>
<pre><code class="language-r">str(castillovar_t)</code></pre>
























<p><span class="small-caps-alt">Novel research ideas</span> in the field of Deep Learning are generally implemented using a combination of native framework operators. While convenient, this approach often requires the creation (and/or movement) of many temporary tensors, which can hurt the performance of neural networks at scale. These issues can be mitigated by writing specialized GPU kernels, but doing so can be surprisingly difficult due to the many intricacies of GPU programming.<span class="js-rfref" data-id="gray-2017"></span><span class="js-rfref" data-id="kerr-2020"></span><span class="js-rfref" data-id="yan-2020"></span> And, although a variety of systems have recently emerged<span class="js-rfref" data-id="cutlass"></span><span class="js-rfref" data-id="tvm"></span> to make this process easier, we have found them to be either too verbose, lack flexibility or generate code noticeably slower than our hand-tuned baselines. This has led us to extend and improve Triton<span class="js-rfref" data-id="tillet-2019"></span>, a recent language and compiler whose original creator now works at OpenAI.</p>
<h2 id="the-challenges-of-gpu-programming">The Challenges of GPU Programming</h2>
<p>The architecture of modern GPUs can be roughly divided into three major components—DRAM, SRAM and ALUs—each of which must be considered when optimizing CUDA code:</p>
<ul>
<li>Memory transfers from DRAM must be <em>coalesced</em> into large transactions to leverage the large bus width of modern memory interfaces.</li>
<li>Data must be manually stashed to SRAM prior to being re-used, and managed so as to minimize shared memory bank conflicts upon retrieval.</li>
<li>Computations must be partitioned and scheduled carefully, both across and within Streaming Multiprocessors (SMs), so as to promote instruction/thread-level parallelism and leverage special-purpose ALUs (e.g., tensor cores).</li>
</ul>
<div id="gpuarchitecture">
<div class="wide mt-2 mb-0">
<div class="mx-xl-auto" style="max-width:795px">
<img class="d-none d-sm-block mb-0" src="https://cdn.openai.com/triton/assets/gpu-architecture.svg">
<img class="d-block d-sm-none mb-0" src="https://cdn.openai.com/triton/assets/gpu-architecture-small.svg">
</div>
</div>
<div class="caption mb-2 mt-0.5">Basic architecture of a GPU.</div>
</div><!-- end #gpuarchitecture -->
<p>Reasoning about all these factors can be challenging, even for seasoned CUDA programmers with many years of experience. The purpose of Triton is to fully automate these optimizations, so that developers can better focus on the high-level logic of their parallel code. Triton aims to be broadly applicable, and therefore does not automatically schedule work across SMs -- leaving some important algorithmic considerations (e.g. tiling, inter-SM synchronization) to the discretion of developers.</p>
<table>
<thead>
<tr>
<th></th>
<th>CUDA</th>
<th>Triton</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Coalescing</td>
<td><span class="color-fg-50">Manual</span></td>
<td>Automatic</td>
</tr>
<tr>
<td>Shared Memory Management</td>
<td><span class="color-fg-50">Manual</span></td>
<td>Automatic</td>
</tr>
<tr>
<td>Scheduling (Within SMs)</td>
<td><span class="color-fg-50">Manual</span></td>
<td>Automatic</td>
</tr>
<tr>
<td>Scheduling (Across SMs)</td>
<td><span class="color-fg-50">Manual</span></td>
<td><span class="color-fg-50">Manual</span></td>
</tr>
</tbody>
</table>
<div class="caption">Compiler optimizations in CUDA vs Triton.</div>
<h2 id="programming-model">Programming Model</h2>
<p>Out of all the Domain Specific Languages and JIT-compilers available, Triton is perhaps most similar to Numba: kernels are defined as decorated Python functions, and launched concurrently with different <code>program_id</code>’s on a grid of so-called <em>instances</em>. However, as shown in the code snippet below, the resemblance stops there: Triton exposes intra-instance parallelism  via operations on <em>blocks</em>—small arrays whose dimensions are powers of two—rather than a Single Instruction, Multiple Thread (SIMT)<span class="js-rfref" data-id="lin-2018"></span> execution model. In doing so, Triton effectively abstracts away all the issues related to concurrency <em>within</em> CUDA thread blocks (e.g., memory coalescing, shared memory synchronization/conflicts,  tensor core scheduling).</p>
<style>
#triton-vector-add .code-toolbar {
   height: 100%;
}
#triton-vector-add pre {
   margin-top: 0;
   margin-bottom: 0;
   height: 100%;
}
</style>
<div class="wide mb-0.5" id="triton-vector-add">
<div class="row"><div class="col-12 col-xl-10 mx-xl-auto">
<div class="row">
<div class="col-12 col-md-6 mb-1">
<pre><code class="language-python">BLOCK = 512

# This is a GPU kernel in Numba.
# Different instances of this
# function may run in parallel.
@jit
def add(X, Y, Z, N):
   # In Numba/CUDA, each kernel 
   # instance itself uses an SIMT execution
   # model, where instructions are executed in
   # parallel for different values of threadIdx
   tid = threadIdx.x
   bid = blockIdx.x
   # scalar index
   idx = bid * BLOCK + tid
   if id &lt; N:
     # There is no pointer in Numba.
     # Z,X,Y are dense tensors
     Z[idx] = X[idx] + Y[idx]


...
grid = (ceil_div(N, BLOCK),)
block = (BLOCK,)
add[grid, block](x, y, z, x.shape[0])
</code></pre>
</div>
<div class="col-12 col-md-6 mb-1">
<pre><code class="language-python">BLOCK = 512

# This is a GPU kernel in Triton.
# Different instances of this
# function may run in parallel.
@jit
def add(X, Y, Z, N):
   # In Triton, each kernel instance
   # executes block operations on a
   # single thread: there is no construct
   # analogous to threadIdx
   pid = program_id(0)
   # block of indices
   idx = pid * BLOCK + arange(BLOCK)
   mask = idx &lt; N
   # Triton uses pointer arithmetics  
   # rather than indexing operators
   x = load(X + idx, mask=mask)
   y = load(Y + idx, mask=mask)
   store(Z + idx, x + y, mask=mask)


...
grid = (ceil_div(N, BLOCK),)
# no thread-block
add[grid](x, y, z, x.shape[0])
</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="caption mb-2">Vector addition in Triton.</div>
<p>While this may not be particularly helpful for embarrassingly parallel (i.e., element-wise) computations, it can greatly simplify the development of more complex GPU programs.</p>
<p>Consider for example the case of a fused softmax kernel (below) in which each instance normalizes a different row of the given input tensor  $X \in \mathbb{R}^{M \times N}$. Standard CUDA implementations of this parallelization strategy can be challenging to write, requiring explicit synchronization between threads as they concurrently reduce the same row of $X$. Most of this complexity goes away with Triton, where each kernel instance loads the row of interest and normalizes it sequentially using NumPy-like primitives.</p>
<pre><code class="language-python">import triton
import triton.language as tl

@triton.jit
def softmax(Y, stride_ym, stride_yn, X, stride_xm, stride_xn, M, N):
    # row index
    m = tl.program_id(0)
    # col indices
    # this specific kernel only works for matrices that 
    # have less than BLOCK_SIZE columns
    BLOCK_SIZE = 1024
    n = tl.arange(0, BLOCK_SIZE)
    # the memory address of all the elements
    # that we want to load can be computed as follows
    X = X + m * stride_xm + n * stride_xn
    # load input data; pad out-of-bounds elements with 0 
    x = tl.load(X, mask=n &lt; N, other=-float('inf'))
    # compute numerically-stable softmax
    z = x - tl.max(x, axis=0)
    num = tl.exp(z)
    denom = tl.sum(num, axis=0)
    y = num / denom
    # write back to Y
    Y = Y + m * stride_ym + n * stride_yn
    tl.store(Y, y, mask=n &lt; N)

import torch
# Allocate input/output tensors
X = torch.normal(0, 1, size=(583, 931), device='cuda')
Y = torch.empty_like(X)
# SPMD launch grid
grid = (X.shape[0], )
# enqueue GPU kernel
softmax[grid](Y, Y.stride(0), Y.stride(1), 
              X, X.stride(0), X.stride(1),
              X.shape[0]    , X.shape[1])
</code></pre>
<div class="caption">Fused softmax in Triton.</div>
<p>Note that the Triton JIT treats X and Y as <em>pointers</em> rather than tensors; we felt like retaining low-level control of memory accesses was important to address more complex data structures (e.g., block-sparse tensors).</p>
<p>Importantly, this particular implementation of softmax keeps the rows of $X$ in SRAM throughout the entire normalization process, which maximizes data reuse when applicable (~&lt;32K columns). This differs from PyTorch’s internal CUDA code, whose use of temporary memory makes it more general but significantly slower (below). The bottom line here is not that Triton is inherently better, but that it simplifies the development of specialized kernels that can be much faster than those found in general-purpose libraries.</p>
<div id="a100" class="mt-1.5 mb-1"></div>
<div class="caption">A100 performance of fused softmax for M=4096.</div>
<p>The lower performance of the Torch (v1.9) JIT highlights the difficulty of automatic CUDA code generation from sequences of high-level tensor operations.</p>
<pre><code class="language-python">@torch.jit.script
def softmax(x):
    x_max = x.max(dim=1)[0]
    z = x - x_max[:, None]
    numerator = torch.exp(x)
    denominator = numerator.sum(dim=1)
    return numerator / denominator[:, None]
</code></pre>
<div class="caption">Fused softmax with the Torch JIT.</div>
<h2 id="matrix-multiplication">Matrix Multiplication</h2>
<p>Being able to write fused kernels for element-wise operations and reductions is important, but not sufficient given the prominence of matrix multiplication tasks in neural networks. As it turns out, Triton also works very well for those, achieving peak performance with just ~25 lines of Python code. On the other hand, implementing something similar in CUDA would take <a href="https://github.com/openai/blocksparse/blob/master/src/matmul_op_gpu.cu">a lot more effort</a> and would even be likely to achieve lower performance.</p>
<pre><code class="language-python">@triton.jit
def matmul(A, B, C, M, N, K, stride_am, stride_ak, 
            stride_bk, stride_bn, stride_cm, stride_cn,
            **META):
    # extract metaparameters
    BLOCK_M, GROUP_M = META['BLOCK_M'], META['GROUP_M']
    BLOCK_N = META['BLOCK_N']
    BLOCK_K = META['BLOCK_K']
    # programs are grouped together to improve L2 hit rate
    _pid_m = tl.program_id(0)
    _pid_n = tl.program_id(1)
    pid_m = _pid_m // GROUP_M
    pid_n = (_pid_n * GROUP_M) + (_pid_m % GROUP_M)
    # rm (resp. rn) denotes a range of indices
    # for rows (resp. col) of C
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    # rk denotes a range of indices for columns 
    # (resp. rows) of A (resp. B)
    rk = tl.arange(0, BLOCK_K)
    # the memory addresses of elements in the first block of
    # A and B can be computed using numpy-style broadcasting
    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk [:, None] * stride_bk  + rn[None, :] * stride_bn)
    # initialize and iteratively update accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    for k in range(K, 0, -BLOCK_K):
        a = tl.load(A)
        b = tl.load(B)
        # block level matrix multiplication
        acc += tl.dot(a, b)
        # increment pointers so that the next blocks of A and B
        # are loaded during the next iteration
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk
    # fuse leaky ReLU if desired
    # acc = tl.where(acc &gt;= 0, acc, alpha * acc)
    # write back result
    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)
    mask = (rm[:, None] &lt; M) &amp; (rn[None, :] &lt; N)
    tl.store(C, acc, mask=mask)
</code></pre>
<div class="caption">Matrix multiplication in Triton.</div>
<p>One important advantage of handwritten matrix multiplication kernels is that they can be customized as desired to accommodate fused transformations of their inputs (e.g., slicing) and outputs (e.g., Leaky ReLU). Without a system like Triton, non-trivial modifications of matrix multiplication kernels would be out-of-reach for developers without exceptional GPU programming expertise.</p>
<div id="v100" class="mt-1.5 mb-1"></div>
<div class="caption">V100 tensor-core performance of matrix multiplication with appropriately tuned values for BLOCK$_M$, BLOCK$_N$, BLOCK$_K$, GROUP$_M$.</div>
<h2 id="high-level-system-architecture">High-Level System Architecture</h2>
<p>The good performance of Triton comes from a modular system architecture centered around Triton-IR, an LLVM-based intermediate representation in which multi-dimensional blocks of values are first-class citizens.</p>
<div id="tritonarchitecture">
<!-- triton architecture diagram, small -->
<div class="d-block d-md-none my-3">
<div class="row no-gutters">
<div class="col-auto small-copy font-bold text-center" style="width:100px">
<div class="bg-cool-gray-0.5 text-center d-flex align-items-center" style="height:35px"><div class="w-100">Python</div></div>
<div><img src="https://cdn.openai.com/triton/assets/arrow-down-ast.svg" class="mb-0" style="height:141px"></div>
<div class="bg-cool-gray-0.5 text-center d-flex align-items-center" style="height:35px"><div class="w-100">Triton-IR</div></div>
<div><img src="https://cdn.openai.com/triton/assets/arrow-down-triton.svg" class="mb-0" style="height:141px"></div>
<div class="bg-cool-gray-0.5 text-center d-flex align-items-center" style="height:35px"><div class="w-100">LLVM-IR</div></div>
<div><img src="https://cdn.openai.com/triton/assets/arrow-down-libllvm.svg" class="mb-0" style="height:141px"></div>
<div class="bg-cool-gray-0.5 text-center d-flex align-items-center" style="height:35px"><div class="w-100">PTX</div></div>
</div>
<div class="col-auto">
<div><div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/zoom-python-small.svg');height:126px;width:60px;margin-left:5px;margin-bottom:50px"></div></div>
<div><div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/zoom-triton-small.svg');height:302px;width:60px;margin-left:5px;margin-bottom:50px"></div></div>
<div><div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/zoom-triton-small.svg');height:302px;width:60px;margin-left:5px"></div></div>
</div>
<div class="col font-xxsmall">
<pre class="language-python my-0" tabindex="0" style="white-space:pre-wrap;border-radius:unset;height:126px"><code class="language-python">@jit
def add(X, Y, Z, N):
   pid = program_id(0)
   idx= pid * 512 + arange(512)
   mask = idx < N
   x = load(X + idx, mask=mask)
   y = load(Y + idx, mask=mask)
   store(Z + idx, x + y, mask=mask)
</code></pre>
<div><img src="https://cdn.openai.com/triton/assets/arrow-down.svg" class="mb-0" style="height:50px"></div>
<pre class="language-python my-0" tabindex="0" style="white-space:pre-wrap;border-radius:unset;height:302px"><code class="language-python">def void add(i32* X .aligned(16) , i32* Y .aligned(16) , i32* Z .aligned(16) , i32 N .multipleof(2) )
{
entry:
  %0 = get_program_id[0] i32;
  %1 = mul i32 %0, 512;
  %3 = make_range[0 : 512] i32<512>;
  %4 = splat i32<512> %1;
  %6 = add i32<512> %4, %3;
  %9 = splat i32<512> N;
  %11 = icmp_slt i1<512> %6, %9;
  %14 = splat i32*<512> X;
  %16 = getelementptr i32*<512> %14, %6;
  %19 = broadcast i1<512> %11;
  %21 = splat i32<512> undef;
  %22 = masked_load i32<512> %16, %19, %21;
  %26 = splat i32*<512> Y;
  %28 = getelementptr i32*<512> %26, %6;
  %31 = broadcast i1<512> %11;
  %33 = splat i32<512> undef;
  %34 = masked_load i32<512> %28, %31, %33;
  %38 = splat i32*<512> Z;
  %40 = getelementptr i32*<512> %38, %6;
  %43 = add i32<512> %22, %34;
  %46 = broadcast i32<512> %43;
  %48 = broadcast i1<512> %11;
  masked_store void %40, %46, %48;
  ret void;
}
</code></pre>
<div><img src="https://cdn.openai.com/triton/assets/arrow-down.svg" class="mb-0" style="height:50px"></div>
<pre class="language-python my-0" tabindex="0" style="white-space:pre-wrap;border-radius:unset;height:302px"><code class="language-python">.visible .entry add(
    .param .u64 add_param_0, .param .u64 add_param_1,
    .param .u64 add_param_2, .param .u32 add_param_3
)
.maxntid 128, 1, 1
{
    .reg .pred     %p<4>;
    .reg .b32     %r<18>;
    .reg .b64     %rd<8>;
    ld.param.u64     %rd4, [add_param_0];
    ld.param.u64     %rd5, [add_param_1];
    mov.u32     %r13, %tid.x;
    ld.param.u32     %r14, [add_param_3];
    shl.b32     %r15, %r13, 2;
    mov.u32     %r16, %ctaid.x;
    mad.lo.s32     %r17, %r16, 512, %r15;
    setp.ge.s32     %p3, %r17, %r14;
    setp.lt.s32     %p1, %r17, %r14;
    mul.wide.s32     %rd7, %r17, 4;
    add.s64     %rd2, %rd4, %rd7;
    @%p1 ld.global.cg.v4.b32 {%r5,%r6,%r7,%r8}, [ %rd2 + 0];
    add.s64     %rd3, %rd5, %rd7;
    @%p1 ld.global.cg.v4.b32 {%r9,%r10,%r11,%r12}, [ %rd3 + 0];
    @%p3 bra     LBB0_2;
    ld.param.u64     %rd6, [add_param_2];
    add.s64     %rd1, %rd6, %rd7;
    add.s32     %r1, %r5, %r9;
    add.s32     %r2, %r6, %r10;
    add.s32     %r3, %r7, %r11;
    add.s32     %r4, %r8, %r12;
    st.global.v4.u32     [%rd1], {%r1, %r2, %r3, %r4};
LBB0_2:
    ret;
}
</code></pre>
</div>
</div><!-- end .row -->
</div><!-- end .d-block .d-md-none -->
<!-- triton architecture diagram -->
<div class="d-none d-md-block wide">
<div class="row align-items-center no-gutters small-copy font-bold text-center">
<div class="col">
  <div class="bg-cool-gray-0.5 px-1/3 pt-1/3 pb-0.25">Python</div>
</div>
<div class="col-auto"><img src="https://cdn.openai.com/triton/assets/arrow-right-ast.svg" class="mb-0" style="width:100px"></div>
<div class="col">
  <div class="bg-cool-gray-0.5 px-1/3 pt-1/3 pb-0.25">Triton-IR</div>
</div>
<div class="col-auto"><img src="https://cdn.openai.com/triton/assets/arrow-right-triton.svg" class="mb-0" style="width:100px"></div>
<div class="col">
  <div class="bg-cool-gray-0.5 px-1/3 pt-1/3 pb-0.25">LLVM-IR</div>
</div>
<div class="col-auto"><img src="https://cdn.openai.com/triton/assets/arrow-right-libllvm.svg" class="mb-0" style="width:100px"></div>
<div class="col">
  <div class="bg-cool-gray-0.5 px-1/3 pt-1/3 pb-0.25">PTX</div>
</div>
</div><!-- end .row -->
<div class="row no-gutters font-xxsmall">
<div class="col-auto" style="width:calc(20% - 20px)">
<div><div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/zoom-python.svg');height:60px;margin-top:5px"></div></div>
<pre class="language-python my-0" tabindex="0" style="white-space:pre-wrap;border-radius:unset"><code class="language-python">@jit
def add(X, Y, Z, N):
   pid = program_id(0)
   idx= pid * 512 + arange(512)
   mask = idx < N
   x = load(X + idx, mask=mask)
   y = load(Y + idx, mask=mask)
   store(Z + idx, x + y, mask=mask)
</code></pre></div>
<div class="col-auto"><img src="https://cdn.openai.com/triton/assets/arrow-right.svg" style="width:50px;margin-top:140px"></div>
<div class="col-auto" style="width:calc(40% - 40px)">
<div><div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/zoom-triton.svg');height:60px;margin-top:5px"></div></div>
<pre class="language-python my-0" tabindex="0" style="white-space:pre-wrap;border-radius:unset"><code class="language-python">def void add(i32* X .aligned(16) , i32* Y .aligned(16) , i32* Z .aligned(16) , i32 N .multipleof(2) )
{
entry:
  %0 = get_program_id[0] i32;
  %1 = mul i32 %0, 512;
  %3 = make_range[0 : 512] i32<512>;
  %4 = splat i32<512> %1;
  %6 = add i32<512> %4, %3;
  %9 = splat i32<512> N;
  %11 = icmp_slt i1<512> %6, %9;
  %14 = splat i32*<512> X;
  %16 = getelementptr i32*<512> %14, %6;
  %19 = broadcast i1<512> %11;
  %21 = splat i32<512> undef;
  %22 = masked_load i32<512> %16, %19, %21;
  %26 = splat i32*<512> Y;
  %28 = getelementptr i32*<512> %26, %6;
  %31 = broadcast i1<512> %11;
  %33 = splat i32<512> undef;
  %34 = masked_load i32<512> %28, %31, %33;
  %38 = splat i32*<512> Z;
  %40 = getelementptr i32*<512> %38, %6;
  %43 = add i32<512> %22, %34;
  %46 = broadcast i32<512> %43;
  %48 = broadcast i1<512> %11;
  masked_store void %40, %46, %48;
  ret void;
}
</code></pre></div>
<div class="col-auto"><img src="https://cdn.openai.com/triton/assets/arrow-right.svg" style="width:50px;margin-top:140px"></div>
<div class="col-auto" style="width:calc(40% - 40px)">
<div><div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/zoom-ptx.svg');height:60px;margin-top:5px"></div></div>
<pre class="language-python my-0" tabindex="0" style="white-space:pre-wrap;border-radius:unset"><code class="language-python">.visible .entry add(
    .param .u64 add_param_0, .param .u64 add_param_1,
    .param .u64 add_param_2, .param .u32 add_param_3
)
.maxntid 128, 1, 1
{
    .reg .pred     %p<4>;
    .reg .b32     %r<18>;
    .reg .b64     %rd<8>;
    ld.param.u64     %rd4, [add_param_0];
    ld.param.u64     %rd5, [add_param_1];
    mov.u32     %r13, %tid.x;
    ld.param.u32     %r14, [add_param_3];
    shl.b32     %r15, %r13, 2;
    mov.u32     %r16, %ctaid.x;
    mad.lo.s32     %r17, %r16, 512, %r15;
    setp.ge.s32     %p3, %r17, %r14;
    setp.lt.s32     %p1, %r17, %r14;
    mul.wide.s32     %rd7, %r17, 4;
    add.s64     %rd2, %rd4, %rd7;
    @%p1 ld.global.cg.v4.b32 {%r5,%r6,%r7,%r8}, [ %rd2 + 0];
    add.s64     %rd3, %rd5, %rd7;
    @%p1 ld.global.cg.v4.b32 {%r9,%r10,%r11,%r12}, [ %rd3 + 0];
    @%p3 bra     LBB0_2;
    ld.param.u64     %rd6, [add_param_2];
    add.s64     %rd1, %rd6, %rd7;
    add.s32     %r1, %r5, %r9;
    add.s32     %r2, %r6, %r10;
    add.s32     %r3, %r7, %r11;
    add.s32     %r4, %r8, %r12;
    st.global.v4.u32     [%rd1], {%r1, %r2, %r3, %r4};
LBB0_2:
    ret;
}
</code></pre></div>
</div><!-- end .row -->
</div><!-- end .wide -->
<div class="caption mb-2">High-level architecture of Triton.</div>
</div><!-- end #tritonarchitecture -->
<p>The <code>@triton.jit</code> decorator works by walking the Abstract Syntax Tree (AST) of the provided Python function so as to generate Triton-IR on-the-fly using a common SSA construction algorithm.<span class="js-rfref" data-id="braun-2013"></span> The resulting IR code is then simplified, optimized and automatically parallelized by our compiler backend, before being converted into high-quality LLVM-IR—and eventually PTX—for execution on recent NVIDIA GPUs. CPUs and AMD GPUs are not supported at the moment, but we welcome community contributions aimed at addressing this limitation.</p>
<h2 id="compiler-backend">Compiler Backend</h2>
<p>We have found that the use of blocked program representations via Triton-IR allows our compiler to automatically perform a wide variety of important program optimizations. For example, data can be automatically stashed to shared memory by looking at the operands of computationally intensive block-level operations (e.g., <code>tl.dot</code>)—and allocated/synchronized using standard liveness analysis techniques.</p>
<div class="my-1.5" id="memory">
<div class="row no-gutters align-items-center mb-0.5">
<div class="col-12 col-sm">
<div>
<img src="https://cdn.openai.com/triton/assets/memory-intervals.svg">
</div>
</div>
<div class="col-12 d-sm-none my-n0.5"><img src="https://cdn.openai.com/triton/assets/arrow-down.svg" style="height:50px;margin-left:143.5px"></div>
<div class="col-auto d-none d-sm-block"><img src="https://cdn.openai.com/triton/assets/arrow-right.svg" class="mt-n10/12 position-relative" style="width:50px;left: calc(var(--v) * -0.3);"></div>
<div class="col-12 col-sm">
<img src="https://cdn.openai.com/triton/assets/memory-capacity.svg">
</div>
</div><!-- end .row -->
<div class="caption">The Triton compiler allocates shared memory by analyzing the live range of block variables used in computationally intensive operations.</div>
</div><!-- end #memory -->
<p>On the other hand, Triton programs can be efficiently and automatically parallelized both (1) across SMs by executing different kernel instances concurrently, and (2) within SMs by analyzing the iteration space of each block-level operation and partitioning it adequately across different SIMD units, as shown below.</p>
<div id="parallelization">
<!-- parallelization diagram, small -->
<div class="d-block d-md-none wide mb-2.5 small-copy">
<div class="row">
<div class="col-8">
<div class="row">
<div class="col-6">
<div class="mb-0.25">Element-wise</div>
<div class="mb-1"><pre class="language-python my-0 w-100" tabindex="0" style=""><code class="language-python">S1 float A[4,4] = ...
S2 float B[4,4] = ...
S3 float C[4,4] = A + B
</code></pre></div>
</div>
<div class="col-6">
<div class="mb-0.25">FP16 matrix multiplication</div>
<div class="mb-1"><pre class="language-python my-0 w-100" tabindex="0" style=""><code class="language-python">S1 half A[4,2] = ...
S2 half B[2,2] = ...
S3 float C[4,2] = dot(A,B)
</code></pre></div>
</div>
</div>
</div>
<div class="col-4">
<ol class="list-lower-latin list-indented color-fg-50 mt-10/12" start="1"><li>Definition of a Triton program <em>P</em> composed of three statements <code>S1</code>, <code>S2</code>,&nbsp;<code>S3</code></li></ol>
</div>
<div class="col-8 mt-n0.5 mb-0.5"><img src="https://cdn.openai.com/triton/assets/parallelization-arrow-down.svg" class="mb-0"></div>
</div><!-- end .row -->
<div class="row">
<div class="col-8">
<div class="row">
<div class="col-6">
<div class="mb-0.25">Vectorized</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-vectorized-space.svg" class="mb-1">
</div>
<div class="col-6">
<div class="mb-0.25">Tensorized</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-tensorized-space.svg" class="mb-1">
</div>
</div>
</div>
<div class="col-4">
<ol class="list-lower-latin list-indented color-fg-50 mt-10/12" start="2"><li>Iteration space of <code>S3</code></li></ol>
</div>
<div class="col-8 mt-n0.5 mb-0.5"><img src="https://cdn.openai.com/triton/assets/parallelization-arrow-down.svg" class="mb-0"></div>
</div><!-- end .row -->
<div class="row">
<div class="col-8">
<div class="mb-0.25">SM</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-sm-small.svg" class="mb-0">
<div class="bg-cover" style="background-image:url('https://cdn.openai.com/triton/assets/parallelization-zoom-down.svg');height:50px"></div>
</div>
<div class="col-4">
<ol class="list-lower-latin list-indented color-fg-50 mt-10/12" start="3"><li class="no-widow">Mapping of <code>S3</code> onto a Stream Multiprocessor (SM)</li></ol>
</div>
</div><!-- end .row -->
<div class="row">
<div class="col-8">
<div class="mb-0.25">GPU</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-gpu-small.svg" class="mb-0">
</div>
<div class="col-4">
<ol class="list-lower-latin list-indented color-fg-50 mt-10/12" start="4"><li>Mapping of <em>P</em> onto the GPU</li></ol>
</div>
</div><!-- end .row -->
</div><!-- end .wide -->
<!-- parallelization diagram -->
<div class="d-none d-md-block wide mb-1 small-copy">
<div class="row"><div class="col-12 col-xl-10 mx-xl-auto">
<div class="row no-gutters line-height-1.2">
<div class="col" style="width:21.25%">
<div class="mb-0.25">Element-wise</div>
<div class="aspect-2/1 d-flex mb-1"><pre class="language-python my-0 w-100" tabindex="0" style=""><code class="language-python">S1 float A[4,4] = ...
S2 float B[4,4] = ...
S3 float C[4,4] = A + B
</code></pre></div>
<div class="mb-0.25">FP16 matrix <span class="d-lg-none">mult.</span><span class="d-none d-lg-inline">multiplication</span></div>
<div class="aspect-2/1 d-flex mb-1"><pre class="language-python my-0 w-100" tabindex="0" style=""><code class="language-python">S1 half A[4,2] = ...
S2 half B[2,2] = ...
S3 float C[4,2] = dot(A,B)
</code></pre></div>
</div>
<div class="col-auto" style="width:5%"><img src="https://cdn.openai.com/triton/assets/parallelization-arrow-right.svg" class="mt-1 mb-0"></div>
<div class="col" style="width:21.25%">
<div class="mb-0.25">Vectorized</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-vectorized-space.svg" class="mb-1">
<div class="mb-0.25">Tensorized</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-tensorized-space.svg" class="mb-1">
</div>
<div class="col-auto" style="width:5%"><img src="https://cdn.openai.com/triton/assets/parallelization-arrow-right.svg" class="mt-1 mb-0"></div>
<div class="col" style="width:21.25%">
<div class="mb-0.25">SM</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-sm.svg" class="mb-1">
</div>
<div class="col-auto" style="width:5%">
  <div class="bg-cover mt-1" style="background-image:url('https://cdn.openai.com/triton/assets/parallelization-zoom-right.svg');width:100%;height:calc(100% - var(--v) * 2);"></div>
</div>
<div class="col" style="width:21.25%">
<div class="mb-0.25">GPU</div>
<img src="https://cdn.openai.com/triton/assets/parallelization-gpu.svg" class="mb-1">
</div>
</div>
<div class="row no-gutters">
<div class="col">
<ol class="list-lower-latin list-indented color-fg-50" start="1"><li>Definition of a Triton program <em>P</em> composed of three statements <code>S1</code>, <code>S2</code>,&nbsp;<code>S3</code></li></ol>
</div>
<div class="col-auto" style="width:5%"></div>
<div class="col">
<ol class="list-lower-latin list-indented color-fg-50" start="2"><li>Iteration space of <code>S3</code></li></ol>
</div>
<div class="col-auto" style="width:5%"></div>
<div class="col">
<ol class="list-lower-latin list-indented color-fg-50" start="3"><li class="no-widow">Mapping of <code>S3</code> onto a Stream Multiprocessor (SM)</li></ol>
</div>
<div class="col-auto" style="width:5%"></div>
<div class="col">
<ol class="list-lower-latin list-indented color-fg-50" start="4"><li>Mapping of <em>P</em> onto the GPU</li></ol>
</div>
</div><!-- end .row -->
</div><!-- end .wide -->
</div></div><!-- end wrap -->
<div class="caption">Automatic parallelization in Triton. Each block-level operation defines a blocked iteration space that is automatically parallelized to make use of the resources available on a Streaming Multiprocessor (SM).</div>
</div><!-- end #parallelization -->
<h2 id="contributing">Contributing</h2>
<p>We intend for Triton to become a community-driven project. Feel free to fork our repository on <a href="https://github.com/openai/triton">GitHub</a>!</p>
<p><em>If you’re interested in joining our team and working on Triton &amp; GPU kernels, <a href="../../jobs/index.html#acceleration">we’re hiring</a>!</em></p>
<div class="wide">
<div class="mx-auto" style="width:5rem">
  <svg viewBox="0 0 320 364" xmlns="http://www.w3.org/2000/svg"><path d="m309.44 83.52-138.88-80.18a21.13 21.13 0 0 0 -21.12 0l-138.88 80.18a21.15 21.15 0 0 0 -10.56 18.3v160.36a21.15 21.15 0 0 0 10.56 18.3l138.88 80.18a21.13 21.13 0 0 0 21.12 0l138.88-80.18a21.15 21.15 0 0 0 10.56-18.3v-160.36a21.15 21.15 0 0 0 -10.56-18.3z" fill="#21b5c2"/><path d="m270.56 120.11v-.11a2.29 2.29 0 0 0 0-.37 1.57 1.57 0 0 0 -.05-.3 1.28 1.28 0 0 0 -.09-.32 1.57 1.57 0 0 0 -.12-.3 1.51 1.51 0 0 0 -.15-.28c-.06-.09-.11-.19-.17-.27a3.35 3.35 0 0 0 -.43-.49l-.23-.2-.3-.21-.09-.07-107-61.77-.1-.06-.11-.06-.12-.07h-.06l-.37-.15h-.23l-.4-.08h-.91l-.41.08h-.22l-.37.16-.14.07-.11.06-.1.06-107 61.77-.09.07-.3.21-.23.2a3.35 3.35 0 0 0 -.43.49c-.06.08-.11.18-.17.27a1.51 1.51 0 0 0 -.15.28c0 .1-.08.2-.12.3a1.68 1.68 0 0 0 -.09.32 1.5 1.5 0 0 0 0 .3 2.29 2.29 0 0 0 0 .37v124.66a1.6 1.6 0 0 0 .05.21l.06.15a2 2 0 0 0 .1.26 1 1 0 0 0 .12.23 1.14 1.14 0 0 0 .15.23 1.71 1.71 0 0 0 .21.26l.22.24a2.11 2.11 0 0 0 .23.19l.3.21.09.07 107.2 61.89a3 3 0 0 0 .64.27 2.87 2.87 0 0 0 2 0 3 3 0 0 0 .64-.27l107.2-61.89.09-.07a2.73 2.73 0 0 0 .3-.21 2.11 2.11 0 0 0 .23-.19l.22-.24a1.71 1.71 0 0 0 .21-.26 1.14 1.14 0 0 0 .15-.23v-.07a2.12 2.12 0 0 0 .11-.21 1.86 1.86 0 0 0 .1-.25l.06-.17a.67.67 0 0 0 0-.2 2 2 0 0 0 0-.24v-.21a.74.74 0 0 0 0-.21v-.11zm-110.56 181.8-100.49-58 61.39-35.46v35.55a2.16 2.16 0 0 0 0 .36.63.63 0 0 0 0 .3 3 3 0 0 1 .1.32l.11.3.16.29c.05.09.1.18.16.27l.21.25.22.24.24.19.3.22.09.06 35.54 20.52.14.08.18.09a3.21 3.21 0 0 0 .54.22l.21.07a3.86 3.86 0 0 0 .76.1h.14a3 3 0 0 0 .73-.09l.22-.07a3.76 3.76 0 0 0 .55-.23l.13-.05 35.72-20.63.1-.07.28-.2.25-.21.2-.22.23-.27.15-.26c.06-.1.12-.2.17-.3a2.47 2.47 0 0 0 .1-.29 1.7 1.7 0 0 0 .1-.32 1.5 1.5 0 0 0 0-.3 2.29 2.29 0 0 0 0-.37v-35.55l61.41 35.44zm103.84-63.84-61.39-35.44 30.7-17.71.09-.07a1.94 1.94 0 0 0 .29-.21l.24-.2a1.7 1.7 0 0 0 .22-.24l.21-.25c.06-.08.11-.18.17-.27l.15-.28a1.28 1.28 0 0 1 .11-.31 2.81 2.81 0 0 0 .1-.31v-42.07a3.18 3.18 0 0 0 -.06-.62 3.32 3.32 0 0 0 -1.26-2 1.73 1.73 0 0 0 -.31-.22l-35.67-20.67h-.1l-.33-.2-.28-.11-.31-.07h-1.28a1.87 1.87 0 0 0 -.34.08l-.27.1a2.21 2.21 0 0 0 -.35.16h-.09l-30.7 17.72v-70.88l100.48 58zm-136.21-35.46v-.55a.81.81 0 0 0 0-.22v-.2a1.19 1.19 0 0 0 -.08-.2 1.43 1.43 0 0 0 -.08-.22c0-.07-.07-.13-.1-.2v-.09l-.07-.09a1 1 0 0 0 -.11-.17l-.15-.2-.14-.16-.14-.14-.18-.15-.18-.14-.17-.11-.09-.07-34.25-19.7v-33.45l64.75 37.38v74.76l-29-16.75zm-3.35-78.61 34.05 19.67.14.07a2.52 2.52 0 0 0 .54.21l.21.07a3.57 3.57 0 0 0 .73.1h.1a3.57 3.57 0 0 0 .73-.1l.22-.02a4.87 4.87 0 0 0 .53-.21l.14-.06 34.08-19.73 29 16.74-64.75 37.37-64.75-37.38zm39.08 60 64.75-37.38v33.51l-34.11 19.59-.09.07-.17.11-.2.15-.16.14-.13.13c-.06.06-.11.13-.16.19a1.15 1.15 0 0 0 -.14.17c0 .06-.07.12-.11.17a.61.61 0 0 1 -.07.1l-.05.11-.09.18-.09.21a1 1 0 0 1 -.07.23 1.21 1.21 0 0 0 -.05.18.81.81 0 0 0 0 .22v39.92l-29 16.75zm-6.71-49-30.65-17.81h-.1l-.33-.15-.3-.11-.29-.07h-.35-.63-.31l-.32.07-.37.07-.33.15h-.1l-35.72 20.67a2.54 2.54 0 0 0 -.5.36 3.27 3.27 0 0 0 -1.11 2.09v42.2a.6.6 0 0 0 .05.3 1.7 1.7 0 0 0 .1.32.91.91 0 0 0 .11.3l.16.29a2.26 2.26 0 0 0 .16.26 2.75 2.75 0 0 0 .22.26 1.51 1.51 0 0 0 .21.23 2.25 2.25 0 0 0 .24.21l.29.2.09.07 30.69 17.72-61.38 35.45v-116l100.48-58z" fill="#fff"/></svg>
</div>
</div>
<style>
/* prism.scss overwrite */*
.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #FF5828; /* dark-red -> orange */
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #21B5C2; /* medium-blue -> teal  */
}
</style>
<script type="module">
import {Runtime, Inspector, Library} from "https://unpkg.com/@observablehq/runtime@4.12.0/dist/runtime.js";
import notebook_a100 from "https://api.observablehq.com/d/adcb738994e266fd.js?v=3";
import notebook_v100 from "https://api.observablehq.com/d/1f5fc5b6e3174964.js?v=3";


const customWidth = function (selector) {
  return (new Library).Generators.observe(function(change) {
    var width = change(document.querySelector(selector).clientWidth);
    function resized() {
      var w = document.querySelector(selector).clientWidth;
      if (w !== width) change(width = w);
    }
    window.addEventListener("resize", resized);
    return function() {
      window.removeEventListener("resize", resized);
    };
  });
};

const a100_renders = {
  "chart": "#a100",
};
new Runtime(Object.assign(new Library, {width: customWidth("#a100")})).module(notebook_a100, name => {
  const selector = a100_renders[name];
  if (selector) { // key exists
    return new Inspector(document.querySelector(selector));
  } else {
    return true;
  }
});

const v100_renders = {
  "chart": "#v100",
};
new Runtime(Object.assign(new Library, {width: customWidth("#v100")})).module(notebook_v100, name => {
  const selector = v100_renders[name];
  if (selector) { // key exists
    return new Inspector(document.querySelector(selector));
  } else {
    return true;
  }
});

</script>
<footer class="post-footer js-post-footer">
<!-- footer item -->
<div><hr><div class="row" id="acknowledgments">
<div class="col">Acknowledgments</div>
<div class="col">
<p>Da Yan (HKUST), DeepSpeed (Microsoft), Anthropic</p>
</div>
</div></div>
<!-- references footer item -->
<div data-order="-1"><hr><div class="row" id="references">
<div class="col">References</div>
<div class="col">
<ol>
<li class="js-ref" data-id="gray-2017">
<p>Gray, S. (2017). <a href="https://github.com/NervanaSystems/maxas/wiki/SGEMM">SGEMM Walkthrough</a>. <em>URL <a href="https://github.com/NervanaSystems/maxas/wiki/SGEMM">https://github.com/NervanaSystems/maxas/wiki/SGEMM</a></em>.</p>
</li>
<li class="js-ref" data-id="kerr-2020">
<p>Kerr, A. (2020). <a href="https://developer.nvidia.com/gtc/2020/video/s21745-vid">Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100</a>. <em>URL <a href="https://developer.nvidia.com/gtc/2020/video/s21745-vid">https://developer.nvidia.com/gtc/2020/video/s21745-vid</a></em>.</p>
</li>
<li class="js-ref" data-id="yan-2020">
<p>Yan, D., Wang, W., &amp; Chu, X. (2020, May). <a href="https://ieeexplore.ieee.org/abstract/document/9139835/">Demystifying tensor cores to optimize half-precision matrix multiply</a>. In <em>2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</em>. IEEE.</p>
</li>
<li class="js-ref" data-id="cutlass">
<p><a href="https://github.com/NVIDIA/cutlass">NVIDIA CUTLASS</a></p>
</li>
<li class="js-ref" data-id="tvm">
<p><a href="https://github.com/apache/tvm">Apache TVM</a></p>
</li>
<li class="js-ref" data-id="tillet-2019">
<p>Tillet, P., Kung, H. T., &amp; Cox, D. (2019, June). <a href="https://dl.acm.org/doi/abs/10.1145/3315508.3329973">Triton: an intermediate language and compiler for tiled neural network computations</a>. In <em>Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</em> (pp. 10-19).</p>
</li>
<li class="js-ref" data-id="lin-2018">
<p>Lin, Y. &amp; Grover, V. (2018). <a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a>. <em>URL <a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/</a></em>.</p>
</li>
<li class="js-ref" data-id="braun-2013">
<p>Braun, M., Buchwald, S., Hack, S., Leißa, R., Mallon, C., &amp; Zwinkau, A. (2013, March). <a href="https://link.springer.com/chapter/10.1007/978-3-642-37051-9_6">Simple and efficient construction of static single assignment form</a>. In <em>International Conference on Compiler Construction</em> (pp. 102-122). Springer, Berlin, Heidelberg.</p>
</li>
</ol>
</div>
</div></div>
</footer><!--kg-card-end: markdown-->
      </div>
    </section>
  </div>
</section>
  <footer class="post-footer post-footer--authors container js-post-footer-authors">
  <div data-order="0">
    <hr>
    <div class="row" id="authors">
      <div class="col">Authors</div>
      <div class="col js-post-footer-authors-list ">
        <span class="post-author"><a class="fade" href="../authors/philippe/index.html">Philippe Tillet</a></span>
      </div>
    </div>
  </div>
</footer>


<footer class="post-footer post-footer--tags container js-post-footer-tags">
  <div>
    <hr>
    <div class="row" id="tags">
      <div class="col">Filed Under</div>
      <div class="col js-post-footer-tags-list">
        <span class="post-tag"><a class="fade" href="../tags/research/index.html">Research</a></span>
      </div>
    </div>
  </div>
</footer>



</article>
  

  


<footer class="footer container medium-xsmall-copy line-height-1.6">
  <a href="../../index.html" class="d-block footer-logo mb-1.75" style="margin-left:-1px"><IMG SRC="../../cdn/Logo-josue-b.png"></a>
  <div class="row mb-1">
    
    <div class="col-6 col-lg">
      <ul class="mb-2">
        <div class="small-caps mb-1/3">Contacto</div>
        
        <li><a class="fade" href="mailto:esai.huaman@gmail.com">Email</a></li>
        
        <li><a class="fade" href="https://twitter.com/Josuehu_">Twitter</a></li>
      
        <li><a class="fade" href="https://www.linkedin.com/in/esai-huaman-meza/">LinkedIn</a></li>
      
        <li><a class="fade" href="http://calendly.com/josuehuaman">Calendly</a></li>      
 
      </ul>
    </div>
  
    <div class="col-6 col-lg">
      <ul class="mb-2">
        <div class="small-caps mb-1/3">Links</div>
        
        <li><a class="fade" href="https://tinyletter.com/josue">Newsletter</a></li>
        
        <li><a class="fade" href="index.html">Travel Blog</a></li>
      
        <li><a class="fade" href="http://github.com/josuemzx">Github</a></li>
      
        <li><a class="fade" href="https://github.com/josuemzx/blog">Source Code</a></li>          
    </ul>
    </div>
  
    <div class="col-6 col-lg">
      <ul class="mb-2">
        <div class="small-caps mb-1/3">Categorías</div>
        
          <li><a class="fade" href="crypto/index.html">Crypto</li>
        
          <li><a class="fade" href="../../tags/geopolitica/index.html">Geopolítica</a></li>
        
          <li><a class="fade" href="life/index.html">Life</a></li>
        
          <li><a class="fade" href="machine-learning/index.html">Machine Learning</a></li>
       
      </ul>
    </div>
  
    <div class="col-6 col-lg">
      <ul class="mb-2">
        <div class="small-caps mb-1/3">Información</div>
        
          <li><a class="fade" href="index.html">About</a></li>
        
          <li><a class="fade" href="https://josuem.substack.com/p/libros">Libros</a></li>
        
          <li><a class="fade" href="archivos/index.html">Archivos</a></li>       
      </ul>
    </div>
    </div>
  
</div>
<div class="row align-items-center mb-0.125">
  <div class="col-12 col-md mb-0.5">
    <a class="fade" style="margin-top:1px" href="index.html">© Josue Huaman 2022</a>&emsp;<wbr> <!--<a class="fade" style="margin-top:1px" href="privacy/index.html">Privacy&nbsp;Policy</a>&emsp;<wbr><a class="fade" style="margin-top:1px" href="terms/index.html">Terms&nbsp;of&nbsp;Use</a> -->
  </div>
  <div class="col-12 col-md-4 col-lg-4 col-xl-3 mb-0.5">
    <div class="d-block d-md-none" style="font-size:1.333333rem"> <a class="fade icon" href="https://twitter.com/Josuehu_">twitter</a> &ensp; <a class="fade icon" href="https://youtube.com/channel/UCN21zdA_CvhdsCCVTvkCiEQ">youtube</a> &ensp; <a class="fade icon" href="https://github.com/josuemzx/">github</a> &ensp; <a class="fade icon" href="https://soundcloud.com/josuemzx">soundcloud</a> &ensp; <a class="fade icon" href="https://www.linkedin.com/in/esai-huaman-meza/">linkedin</a> &ensp; <a class="fade icon" href="https://facebook.com">facebook</a> </div> <div class="d-none d-md-block" style="font-size:1.166667rem"> <div class="d-flex justify-content-between"> <a class="fade icon" href="https://twitter.com/Josuehu_">twitter</a> &ensp; <a class="fade icon" href="https://youtube.com/channel/UCN21zdA_CvhdsCCVTvkCiEQ">youtube</a> &ensp; <a class="fade icon" href="https://github.com/josuemzx">github</a> &ensp; <a class="fade icon" href="https://soundcloud.com/josuemzx">soundcloud</a> &ensp; <a class="fade icon" href="https://www.linkedin.com/in/esai-huaman-meza/">linkedin</a> &ensp; <a class="fade icon" href="https://facebook.com">facebook</a> </div> </div>
  </div>
</div>
</footer>
<script type="text/javascript" src="../assets/scripts/mainc205.js?v=faca4a392d"></script>


  
</body>
</html>
